# -*- coding: utf-8 -*-
"""HYP_AI Algorithm(Feb 11, 2024) .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IAw4GZSgAxpJpAsxKWXiAokQ-YXlTtzN

#분석개요.
- 헬스브릿지 데이터
- 고혈압 예측모델

2024년 1월 18일

데이터명: d0118
종속변수: 고혈압 (변수명: hyp)
"""

# pandas 명령문 시작
import pandas as pd
import numpy as np

#데이터 불러오기.
from google.colab import drive
drive.mount('/content/drive')

train = pd.read_csv("/content/drive/MyDrive/Health/d0118.csv")

#데이터확
train

#pycaret 라이브러리 설치 - automl 실습에 필요한 라이브러리

!pip install pycaret

#설치후 동작 확인

from pycaret.datasets import get_data

import pandas as pd



print(train.columns)

#학습데이터, 판다스의 데이터프레임으로 바꾸어 준다.

train_columns = pd.DataFrame({'columns':train.columns})
display(train_columns)

#각주 달기.
#train_columns = pd.DataFrame({'columns':train.columns,
#                              'info':['고혈압유무','당뇨유무','고지혈증유무','COPD유무', 'PTSD유무','스트레스(%)','불안','우울','체지방률','근육률','기초대사량','산소포화도','심장박동수','일걸음평균']})

train_columns = pd.DataFrame({'columns':train.columns,
                              'info' : ['HYP', 'Diabetes', 'Hyperlipidemia', 'COPD', 'PTSD', 'Stress(%)', '불안', 'Depression', 'Body fat percentage', 'Body muscle percentage', 'BMR', 'OS', 'HR', 'Average daily step count']})

display(train_columns)

"""# I. 앙상블모델
https://leehah0908.tistory.com/14
"""

#앙상블 모델 예측

import sklearn.metrics as metrics
from sklearn.metrics import f1_score, accuracy_score, recall_score, roc_auc_score, log_loss
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler, RobustScaler

from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

x_train, x_test, y_train, y_test = train_test_split(train,train['hyp'],stratify=train['hyp'], test_size=0.3, random_state=1)

#stratify=train['hyp'] 옵션 : target의 class 비율을 유지

x_train, x_test, y_train, y_test = train_test_split(train,train['hyp'],stratify=train['hyp'], test_size=0.3, random_state=1)

#stratify=train['hyp'] 옵션 : target의 class 비율을 유지




# 'hyp' 열을 삭제한 새로운 x_train
x_train = x_train.drop('hyp', axis=1)

# 'hyp' 열을 삭제한 새로운 x_test
x_test = x_test.drop('hyp', axis=1)

x_train



y_train

print(x_train.head())

x_train = x_train.apply(pd.to_numeric, errors='coerce')

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")

imputer.fit(x_train)

import pandas as pd

# X_train에 누락된 값이 있는지 확인합니다.
missing_values = x_train.isnull().sum()

# 각 열의 누락된 값 수를 출력합니다.
print(missing_values)

"""## Random Forest ; RF"""

# Random Forest; RF

rf = RandomForestClassifier()
rf.fit(x_train, y_train)
rf.predict(x_test)
rf_pred = rf.predict(x_test)

#변수 중요도
feature_importance = pd.DataFrame(rf.feature_importances_.reshape((1, -1)), columns=x_train.columns, index=['feature_importance'])
feature_importance

"""참고 : [링크 텍스트](https://yaeyang0629.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%ED%8C%8C%EC%9D%B4%EC%8D%AC-Random-Forest-Regressor%ED%9A%8C%EA%B7%80)

-> 변수중요도 시각화.

## Gradient Boosting model; GBM
"""

# Gradient Boosting model; GBM

gbm = GradientBoostingClassifier()
gbm.fit(x_train, y_train)
gbm.predict(x_test)
gbm_pred = gbm.predict(x_test)

"""##Extreme Gradient Boosting model; XGBM"""

# Extreme Gradient Boosting model; XGBM
xgbm = XGBClassifier()
xgbm.fit(x_train, y_train) #훈련
xgbm.predict(x_test)
xgbm_pred = xgbm.predict(x_test) #예

"""##Light Gradient Boosting model; LGB"""

#Light Gradient Boosting model; LGBM

lgbm = LGBMClassifier()
lgbm.fit(x_train, y_train)
lgbm.predict(x_test)
lgbm_pred = lgbm.predict(x_test)

"""## Kneighbors"""

# Kneighbors
knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(x_train, y_train)
knn.predict(x_test)
knn_pred = knn.predict(x_test)

#모델 합치기
models = [gbm, xgbm, lgbm, rf, knn]
models_name = ['GBM','XGBM','LGBM','RF','KNN']

# check by model
for model, name in zip(models, models_name):
    print('##########',name,'##############################')
    print(f'정확도: {accuracy_score(y_test, model.predict(x_test)) * 100:.4f}%')
    print(f'재현율: {recall_score(y_test, model.predict(x_test),pos_label=1) * 100:.4f}%')
    print(f'F1_score: {f1_score(y_test, model.predict(x_test)) * 100:.4f}%')
    print(f'ROC_AUC: {roc_auc_score(y_test, model.predict(x_test)) * 100:.4f}%')
    print('')
# 정확도(KNN), 재현율(KNN), F1_score(KNN), ROC_AUC(GBM)
# 현재 결과로는 XGBM = LGBM = RF

"""Soft Voting
- 다수의 분류모델의 예측확률의 평균 값이 가장 높은 레이블 값을 최종 결과로 산추
"""

from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
gbm = GradientBoostingClassifier()

xgbm = XGBClassifier()

lgbm = LGBMClassifier()

knn = KNeighborsClassifier()


#models = [gbm, xgbm, lgbm, rf, knn]
# 소프트 보팅을 위한 VotingClassifier 정의
voting_classifier = VotingClassifier(
    estimators=[('gbm', gbm), ('xgbm', xgbm), ('lgbm', lgbm), ('rf', rf), ('knn', knn)],
    voting='soft'  # 'hard'로 설정하면 하드 보팅
)
# 모델 학습
voting_classifier.fit(x_train, y_train)

"""**보루타 변수선택 코드**

- Boruta 는 랜덤포레스트 기반으로 만들어진 변수 알고리즘
- 학습시 쉐도우 변수데이터 이

참고: https://zephyrus1111.tistory.com/479

# II. 보루타모델

## 1. **Intro**
"""

!pip install boruta
!pip install shap

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

import pandas as pd
import numpy as np
from google.colab import drive

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    f1_score, accuracy_score, recall_score, roc_auc_score, log_loss, precision_score,
    confusion_matrix, classification_report, roc_curve, precision_recall_curve, auc
)

from boruta import BorutaPy
import scipy as sp
from sklearn.utils import shuffle
from tqdm.notebook import tqdm

import shap

# Drive Mount
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Health/d0118.csv')

print(df.columns)

#각주 달기.
df_columns = pd.DataFrame({'columns':df.columns,
                              'info' : ['HYP', 'Diabetes', 'Hyperlipidemia', 'COPD', 'PTSD', 'Stress(%)', '불안', 'Depression', 'Body fat percentage', 'Body muscle percentage', 'BMR', 'OS', 'HR', 'Average daily step count']})



display(df_columns)

display(df_columns)

"""## 2. Models


"""

#stratify=train['hyp'] 옵션 : target의 class 비율을 유지
train_x, test_x, train_y, test_y = train_test_split(df,df['hyp'],stratify=df['hyp'], test_size=0.3, random_state=1)

# 'hyp' drop
train_x = train_x.drop('hyp', axis=1)
test_x = test_x.drop('hyp', axis=1)

'''# type check -> data is numeric
column_types = train_x.dtypes
print(column_types)
'''

''' NA Check -> No NA
# 각 열별로 결측치 여부 확인
missing_values = test_x.isnull().any()

# 결과 출력
print("각 열의 결측치 여부:")
print(missing_values)
'''

"""### a. Feature Importance"""

# Random Forest; RF
rf = RandomForestClassifier()
rf.fit(train_x, train_y)
rf.predict(test_x)
rf_pred = rf.predict(test_x)

# Feature Importance
feature_importance = pd.DataFrame(rf.feature_importances_.reshape((1, -1)), columns=train_x.columns, index=['feature_importance'])
feature_importance

"""###b. Make Function"""

# Gradient Boosting model; GBM
gbm = GradientBoostingClassifier()

# 모델을 train 데이터에 fit
gbm.fit(train_x, train_y)

# test data로 predict
gbm_pred = gbm.predict(test_x)

# 정확도 출력
accuracy = accuracy_score(test_y, gbm_pred)
print(f'Accuracy: {accuracy:.4f}')

# 혼동 행렬 출력
conf_matrix = confusion_matrix(test_y, gbm_pred)
print('Confusion Matrix:')
print(conf_matrix)

# 분류 보고서 출력 (정밀도, 재현율, F1 스코어 등 포함)
class_report = classification_report(test_y, gbm_pred)
print('Classification Report:')
print(class_report)

gbm_probs = gbm.predict_proba(test_x)[:, 1]

fpr, tpr, thresholds = roc_curve(test_y, gbm_probs) # FPR, TPR, thresholds
roc_auc = roc_auc_score(test_y, gbm_probs) # AUC
precision, recall, thresholds = precision_recall_curve(test_y, gbm_probs) # Precision, Recall, thresholds


fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# ROC Curve
axes[0].plot(fpr, tpr, label=f'GBM ROC Curve (AUC = {roc_auc:.2f})', color='b')
axes[0].plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')
axes[0].fill_between(fpr, tpr, color='skyblue', alpha=0.2)
axes[0].set_xlabel('False Positive Rate')
axes[0].set_ylabel('True Positive Rate')
axes[0].set_title('Receiver Operating Characteristic (ROC) Curve')
axes[0].legend()

# Recall Curve
pr_auc = auc(recall, precision)
axes[1].plot(recall, precision, label=f'GBM Recall Curve (AUC = {pr_auc:.2f})', color='g')
axes[1].set_xlabel('Recall')
axes[1].set_ylabel('Precision')
axes[1].set_title('Precision-Recall Curve for GBM')
axes[1].legend()

plt.tight_layout()  # 간격 조정
plt.show()

"""###c. Evaluate 5 models"""

def evaluate_model(model, train_x, test_x, train_y, test_y):
    # 모델 훈련
    model.fit(train_x, train_y)

    # 예측
    y_pred = model.predict(test_x)

    # 성능 지표 출력
    print(f'Accuracy: {accuracy_score(test_y, y_pred):.4f}')
    print('Confusion Matrix:')
    print(confusion_matrix(test_y, y_pred))
    print('Classification Report:')
    print(classification_report(test_y, y_pred))

    # ROC 곡선 및 AUC 시각화
    probs = model.predict_proba(test_x)[:, 1]
    fpr, tpr, _ = roc_curve(test_y, probs)
    roc_auc = auc(fpr, tpr)

    # Precision-Recall 곡선 및 AUC 시각화
    precision, recall, _ = precision_recall_curve(test_y, probs)
    pr_auc = auc(recall, precision)

    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    # ROC Curve
    axes[0].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='b')
    axes[0].plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')
    axes[0].fill_between(fpr, tpr, color='skyblue', alpha=0.2)
    axes[0].set_xlabel('False Positive Rate')
    axes[0].set_ylabel('True Positive Rate')
    axes[0].set_title('Receiver Operating Characteristic (ROC) Curve')
    axes[0].legend()

    # Recall Curve
    axes[1].plot(recall, precision, label=f'Recall Curve (AUC = {pr_auc:.2f})', color='g')
    axes[1].set_xlabel('Recall')
    axes[1].set_ylabel('Precision')
    axes[1].set_title('Precision-Recall Curve')
    axes[1].legend()

    plt.tight_layout()  # 간격 조정
    plt.show()

# Random Forest 모델
rf = RandomForestClassifier()
evaluate_model(rf, train_x, test_x, train_y, test_y)

# Gradient Boosting 모델
gbm = GradientBoostingClassifier()
evaluate_model(gbm, train_x, test_x, train_y, test_y)

# XGBoost 모델
xgb = XGBClassifier()
evaluate_model(xgb, train_x, test_x, train_y, test_y)

# Light Gradient Boosting
lgb = LGBMClassifier()
evaluate_model(lgb, train_x, test_x, train_y, test_y)

# K-neighbors
knn = KNeighborsClassifier(n_neighbors=3)
evaluate_model(knn, train_x, test_x, train_y, test_y)

"""###d. Summary"""

def evaluate_models(models, train_x, test_x, train_y, test_y):
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    for model in models:
        # 모델 훈련
        model.fit(train_x, train_y)

        # 예측
        y_pred = model.predict(test_x)

        # 성능 지표 출력
        print(f'Model: {model.__class__.__name__}')
        print(f'Accuracy: {accuracy_score(test_y, y_pred):.4f}')
        print('Confusion Matrix:')
        print(confusion_matrix(test_y, y_pred))
        print('Classification Report:')
        print(classification_report(test_y, y_pred))

        # ROC 곡선 및 AUC 시각화
        probs = model.predict_proba(test_x)[:, 1]
        fpr, tpr, _ = roc_curve(test_y, probs)
        roc_auc = auc(fpr, tpr)

        # Precision-Recall 곡선 및 AUC 시각화
        precision, recall, _ = precision_recall_curve(test_y, probs)
        pr_auc = auc(recall, precision)

        # ROC 커브
        axes[0].plot(fpr, tpr, label=f'{model.__class__.__name__} (AUC = {roc_auc:.2f})')

        # Recall 커브
        axes[1].plot(recall, precision, label=f'{model.__class__.__name__} (AUC = {pr_auc:.2f})')

    # 플로팅 설정
    axes[0].plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')
    axes[0].fill_between(fpr, tpr, color='skyblue', alpha=0.2)
    axes[0].set_xlabel('False Positive Rate')
    axes[0].set_ylabel('True Positive Rate')
    axes[0].set_title('Receiver Operating Characteristic (ROC) Curve')
    axes[0].legend()

    axes[1].set_xlabel('Recall')
    axes[1].set_ylabel('Precision')
    axes[1].set_title('Precision-Recall Curve')
    axes[1].legend()

    plt.tight_layout()  # 간격 조정
    plt.show()

# Run
rf = RandomForestClassifier()
gbm = GradientBoostingClassifier()
xgb = XGBClassifier()
lgb = LGBMClassifier()
knn = KNeighborsClassifier(n_neighbors=3)

models = [gbm, xgb, lgb, rf, knn]
evaluate_models(models, train_x, test_x, train_y, test_y)

#모델 합치기
models = [gbm, xgb, lgb, rf, knn]
models_name = ['GBM','XGBM','LGBM','RF','KNN']

# check by model
for model, name in zip(models, models_name):
    print('##########',name,'##############################')
    print(f'정확도: {accuracy_score(test_y, model.predict(test_x)) * 100:.4f}%')
    print(f'정밀도:{precision_score(test_y, model.predict(test_x)) * 100:.4f}%')
    print(f'재현율: {recall_score(test_y, model.predict(test_x),pos_label=1) * 100:.4f}%')
    print(f'F1_score: {f1_score(test_y, model.predict(test_x)) * 100:.4f}%')
    print(f'ROC_AUC: {roc_auc_score(test_y, model.predict(test_x)) * 100:.4f}%')
    print('')

"""##3.Voting

### Soft ver
"""

rf = RandomForestClassifier()
gbm = GradientBoostingClassifier()
xgb = XGBClassifier()
lgb = LGBMClassifier()
knn = KNeighborsClassifier()


# VotingClassifier for Soft voting
voting_classifier = VotingClassifier(
    estimators=[('gbm', gbm), ('xgbm', xgb), ('lgbm', lgb), ('knn', knn)],
    voting='soft'  # 'hard'로 설정하면 하드 보팅
)

# 모델 학습
voting_classifier.fit(train_x, train_y)

# 예측
voting_pred = voting_classifier.predict(test_x)

# 정확도 출력
accuracy = accuracy_score(test_y, voting_pred)
print(f'Voting Classifier Accuracy: {accuracy}')

"""###hard ver"""

rf = RandomForestClassifier()
gbm = GradientBoostingClassifier()
xgb = XGBClassifier()
lgb = LGBMClassifier()
knn = KNeighborsClassifier()


# VotingClassifier for Soft voting
voting_classifier = VotingClassifier(
    estimators=[('gbm', gbm), ('xgbm', xgb), ('lgbm', lgb), ('knn', knn)],
    voting='hard'
)

# 모델 학습
voting_classifier.fit(train_x, train_y)

# 예측
voting_pred = voting_classifier.predict(test_x)

# 정확도 출력
accuracy = accuracy_score(test_y, voting_pred)
print(f'Voting Classifier Accuracy: {accuracy}')

"""## 4.Boruta Algorithm

> Boruta 알고리즘의 시간 복잡도는 O(P⋅N)이다.

> P : 피쳐의 개수 N : 데이터의 행
"""

# data read & rename
df = pd.read_csv('/content/drive/MyDrive/Health/d0118.csv')
df.columns = ['HYP', 'Diabetes', 'Hyperlipidemia', 'COPD', 'PTSD', 'Stress(%)', 'Anxiety', 'Depression', 'Body fat percentage', 'Body muscle percentage', 'BMR', 'OS', 'HR', 'Average daily step count']

#stratify=train['hyp'] 옵션 : target의 class 비율을 유지
X, D_X, Y, D_Y = train_test_split(df,df['HYP'],stratify=df['HYP'], test_size=0.3, random_state=1)

# 'hyp' drop
X = X.drop('HYP', axis=1)
D_X = D_X.drop('HYP', axis=1)

# Initialize
rf = RandomForestClassifier(max_depth=5)
boruta_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=42, max_iter=200)

# Boruta 알고리즘에 데이터를 적용
boruta_selector.fit(X.values, Y.values)

num_columns = df.shape[1]
selected_features = np.where(boruta_selector.support_)[0]

print(f"Number of columns in the DataFrame : {num_columns}")
print("Selected Features num :", boruta_selector.n_features_)
print("Non-selected Feature num :", num_columns - boruta_selector.n_features_)
print("Selected Features Index :", selected_features)
print("Selected Features Ranking :", boruta_selector.ranking_)

# Random Forest 모델에서 특성 중요도 얻기
rf.fit(X.values, Y.values)
importance = rf.feature_importances_

selected_features_indices = np.where(boruta_selector.support_)[0]
selected_features_importance = importance[selected_features_indices]
selected_features_names = X.columns[selected_features_indices]

# sort by importance
sorted_indices = np.argsort(selected_features_importance)[::-1]
sorted_names = selected_features_names[sorted_indices]
sorted_importance = selected_features_importance[sorted_indices]

#pont set
plt.rc('font', family='NanumBarunGothic')
plt.rcParams['axes.unicode_minus'] =False

# plot
plt.figure(figsize=(10, 6))
plt.barh(range(len(selected_features)), sorted_importance, align='center')
plt.yticks(range(len(sorted_importance)), sorted_names)
plt.xlabel('Feature Importance')
plt.title('Important of selected features')
plt.show()

boruta_selector.ranking_

num_columns = df.shape[1]
print(f"Number of columns in the DataFrame : {num_columns}")

print("Selected Features num :", boruta_selector.n_features_)

print("Non-selected Feature num :", num_columns - boruta_selector.n_features_)

selected_features = np.where(boruta_selector.support_)[0]
print("Selected Features Index:", selected_features)

# Random Forest 모델에서 특성 중요도 얻기
rf.fit(X.values, Y.values)
importance = rf.feature_importances_

# 선택된 변수들의 인덱스 가져오기
selected_features_indices = np.where(boruta_selector.support_)[0]

# 선택된 변수들의 중요도 가져오기
selected_features_importance = importance[selected_features_indices]

# 선택된 변수들의 이름 가져오기
selected_features_names = X.columns[selected_features_indices]

# 중요도를 기준으로 내림차순 정렬
sorted_indices = np.argsort(selected_features_importance)[::-1]
sorted_names = selected_features_names[sorted_indices]
sorted_importance = selected_features_importance[sorted_indices]

# 시각화
plt.figure(figsize=(10, 6))
plt.barh(range(len(sorted_importance)), sorted_importance, align='center')
plt.yticks(range(len(sorted_importance)), sorted_names)
plt.xlabel('Feature Importance')
plt.title('Selected Features Importance')
plt.show()

"""#5.SHAP(Shapley Additive exPlanations)"""

# 예제 데이터 생성
df = pd.read_csv('/content/drive/MyDrive/Health/d0118.csv')
#df.columns = ['고혈압', '당뇨', '고지혈증', 'COPD', 'PTSD', '스트레스(%)', '불안', '우울', '체지방률', '근육률', '기초대사량', '산소포화도', '심장박동수', '일걸음평균']

df.columns = ['HYP', 'Diabetes', 'Hyperlipidemia', 'COPD', 'PTSD', 'Stress(%)', 'Anxiety', 'Depression', 'Body fat percentage', 'Body muscle percentage', 'BMR', 'OS', 'HR', 'Average daily step count']


#stratify=train['hyp'] 옵션 : target의 class 비율을 유지
X_forSHAP, TX_forSHAP, Y_forSHAP, TY_forSHAP = train_test_split(df,df['HYP'],stratify=df['HYP'], test_size=0.3, random_state=1)

# 'hyp' drop
X_forSHAP = X_forSHAP.drop('HYP', axis=1)
TX_forSHAP = TX_forSHAP.drop('HYP', axis=1)

# 모델 훈련
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_forSHAP, Y_forSHAP)

# SHAP 값 계산
explainer = shap.TreeExplainer(model)  # 트리 기반 모델의 경우 TreeExplainer 사용
shap_values = explainer.shap_values(TX_forSHAP)

# 특정 샘플의 SHAP 요약 플롯
shap.summary_plot(shap_values[1], TX_forSHAP, feature_names=X_forSHAP.columns)

# 특정 샘플의 force plot
sample_index = 0
shap.force_plot(explainer.expected_value[1], shap_values[1][sample_index, :], TX_forSHAP.iloc[sample_index, :])